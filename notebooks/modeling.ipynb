{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Flatten, Concatenate, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_names = [\n",
    "            'vehicle_mileage', 'vehicle_year', 'vehicle_doors',\n",
    "            'vehicle_length', 'vehicle_trunk_volume', 'vehicle_power_din',\n",
    "            'vehicle_rated_horse_power', 'vehicle_max_power', 'vehicle_consumption',\n",
    "            'vehicle_co2', 'price', 'initial_price', 'vehicle_price_new'\n",
    "        ]\n",
    "categorical_columns_names = [\n",
    "            'customer_type', 'vehicle_category', 'vehicle_make', 'vehicle_model',\n",
    "            'vehicle_version', 'vehicle_gearbox', 'vehicle_energy', 'vehicle_origin', 'vehicle_external_color', 'vehicle_internal_color',\n",
    "            'vehicle_four_wheel_drive', 'vehicle_pollution_norm',\n",
    "            'vehicle_condition', 'vehicle_motorization', 'vehicle_trim_level', \"vehicle_commercial_name\"\n",
    "        ]\n",
    "\n",
    "dataframe = pd.read_csv(\"../data/classified_23_06_2025.csv\", nrows=100)\n",
    "prediction_df = dataframe.sample(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import dataclasses\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class Column(ABC):\n",
    "    @abstractmethod\n",
    "    def transform(self, series: pd.Series) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CategoricalColumn (Column):\n",
    "    name: str\n",
    "    vocabulary: Dict[str, int]\n",
    "    value_used_to_fill_na: str = \"UNK\"  \n",
    "    embedding_dim: int = 32\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.vocabulary = {**self.vocabulary, self.value_used_to_fill_na: len(self.vocabulary)}\n",
    "\n",
    "    @classmethod\n",
    "    def from_vocabulary(cls, name: str, vocabulary: Dict[str, int]) -> \"CategoricalColumn\":\n",
    "        return cls(\n",
    "            name=name,\n",
    "            vocabulary=vocabulary,\n",
    "            embedding_dim=cls.infer_embedding_dim(vocabulary)\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_series(cls, series: pd.Series) -> \"CategoricalColumn\":\n",
    "        series.fillna(cls.value_used_to_fill_na, inplace=True)\n",
    "        vocabulary = {value: index for index, value in enumerate(series.unique())}\n",
    "        return cls(\n",
    "            name=series.name,\n",
    "            vocabulary=vocabulary,\n",
    "            embedding_dim=cls.infer_embedding_dim(vocabulary)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def infer_embedding_dim(vocabulary: Dict[str, int]) -> int:\n",
    "        if len(vocabulary) < 30:\n",
    "            return 10\n",
    "        else:\n",
    "            return 32\n",
    "\n",
    "    def transform(self, series: pd.Series) -> pd.Series:\n",
    "        series.fillna(self.value_used_to_fill_na, inplace=True)\n",
    "        return series.map(self.vocabulary)\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        dict_to_save = dataclasses.asdict(self)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(dict_to_save, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"CategoricalColumn\":\n",
    "        with open(path, \"r\") as f:\n",
    "            return cls(**json.load(f))\n",
    "\n",
    "@dataclass\n",
    "class CategoricalColumns:\n",
    "    columns: Dict[str, CategoricalColumn]\n",
    "\n",
    "    @classmethod\n",
    "    def from_names(cls, dataframe: pd.DataFrame, columns: List[str]) -> \"CategoricalColumns\":\n",
    "        return cls(\n",
    "            columns={column: CategoricalColumn.from_series(dataframe[column]) for column in columns}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_categorical_columns(cls, categorical_columns: List[CategoricalColumn]) -> \"CategoricalColumns\":\n",
    "        return cls(\n",
    "            columns={column.name: column for column in categorical_columns}\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class NumericalColumn(Column):\n",
    "    name: str\n",
    "    value_used_to_fill_na: float | int\n",
    "    mean: float\n",
    "    std: float = 1.0\n",
    "\n",
    "    @classmethod\n",
    "    def from_precomputed(cls, name: str, mean: float, std: float) -> \"NumericalColumn\":\n",
    "        return cls(\n",
    "            name=name,\n",
    "            value_used_to_fill_na=mean,\n",
    "            mean=mean,\n",
    "            std=std\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_series(cls, series: pd.Series) -> \"NumericalColumn\":\n",
    "        mean = series.mean() \n",
    "        series.fillna(mean, inplace=True)\n",
    "        std = series.std()\n",
    "        return NumericalColumn(\n",
    "            name=series.name,\n",
    "            value_used_to_fill_na=mean,\n",
    "            mean=mean,\n",
    "            std=std\n",
    "        )\n",
    "    \n",
    "    def transform(self, series: pd.Series) -> pd.Series:\n",
    "        series.fillna(self.value_used_to_fill_na, inplace=True)\n",
    "        return (series - self.mean) / self.std\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        dict_to_save = dataclasses.asdict(self)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(dict_to_save, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"NumericalColumn\":\n",
    "        with open(path, \"r\") as f:\n",
    "            return NumericalColumn(**json.load(f))\n",
    "\n",
    "@dataclass\n",
    "class NumericalColumns:\n",
    "    columns: Dict[str, NumericalColumn]\n",
    "    numerical_dimensions: int\n",
    "\n",
    "    @classmethod\n",
    "    def from_names(cls, dataframe: pd.DataFrame, columns: List[str]) -> \"NumericalColumns\":\n",
    "        return cls(\n",
    "            columns={column: NumericalColumn.from_series(dataframe[column]) for column in columns},\n",
    "            numerical_dimensions=len(columns)\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_numerical_columns(cls, numerical_columns: List[NumericalColumn]) -> \"NumericalColumns\":\n",
    "        return cls(\n",
    "            columns={column.name: column for column in numerical_columns},\n",
    "            numerical_dimensions=len(numerical_columns)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class DatasetAnalysis:\n",
    "    numerical_columns: NumericalColumns\n",
    "    categorical_columns: CategoricalColumns\n",
    "\n",
    "    def get_analysis(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"numerical_columns_names\": self.numerical_columns,\n",
    "            \"categorical_columns_names\": self.categorical_columns\n",
    "        }\n",
    "\n",
    "class DatasetPreprocessor:\n",
    "    def __init__(self, numerical_columns_names: List[str], categorical_columns_names: List[str]):\n",
    "        self.numerical_columns_names = numerical_columns_names\n",
    "        self.categorical_columns_names = categorical_columns_names\n",
    "\n",
    "    def fit(self, dataframe: pd.DataFrame) -> None:\n",
    "        print(\"Fitting dataset preprocessor\")\n",
    "        self.numerical_columns = NumericalColumns.from_names(dataframe, columns=self.numerical_columns_names)\n",
    "        self.categorical_columns = CategoricalColumns.from_names(dataframe, columns=self.categorical_columns_names)\n",
    "\n",
    "    def get_analysis(self) -> DatasetAnalysis:\n",
    "        return DatasetAnalysis(self.numerical_columns, self.categorical_columns)\n",
    "\n",
    "    def preprocess(self, dataframe: pd.DataFrame) -> Dict[str, pd.Series]:\n",
    "        transformed_data = dataframe[self.numerical_columns_names + self.categorical_columns_names].copy()\n",
    "\n",
    "        for column in self.numerical_columns.columns:\n",
    "            transformed_data[column] = self.numerical_columns.columns[column].transform(transformed_data[column])\n",
    "\n",
    "        for column in self.categorical_columns.columns:\n",
    "            transformed_data[column] = self.categorical_columns.columns[column].transform(transformed_data[column])\n",
    "\n",
    "        return {\n",
    "            \"numerical_inputs_features\": transformed_data[self.numerical_columns_names].values,\n",
    "            **{\n",
    "                feature_name: transformed_data[feature_name].values\n",
    "                for feature_name in self.categorical_columns_names\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def preprocess_target(self, dataframe: pd.DataFrame) -> Dict[str, pd.Series]:\n",
    "        transformed_data = dataframe[self.numerical_columns_names + self.categorical_columns_names].copy()\n",
    "\n",
    "        for column in self.numerical_columns.columns:\n",
    "            transformed_data[column] = self.numerical_columns.columns[column].transform(transformed_data[column])\n",
    "\n",
    "        for column in self.categorical_columns.columns:\n",
    "            transformed_data[column] = self.categorical_columns.columns[column].transform(transformed_data[column])\n",
    "\n",
    "        return {\n",
    "            \"numerical_outputs\": transformed_data[self.numerical_columns_names].values,\n",
    "            **{\n",
    "                feature_name + \"_outputs\": transformed_data[feature_name].values\n",
    "                for feature_name in self.categorical_columns_names\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Move to dedicated infrastructure class\n",
    "    def save(self, directory: str) -> None:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        for column_name, column in self.numerical_columns.columns.items():\n",
    "            column.save(f\"{directory}/{column_name}.json\")\n",
    "\n",
    "        for column_name, column in self.categorical_columns.columns.items():\n",
    "            column.save(f\"{directory}/{column_name}.json\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_columns(cls, numerical_columns: List[NumericalColumn], categorical_columns: List[CategoricalColumn]) -> \"DatasetPreprocessor\":\n",
    "        return cls(numerical_columns, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting dataset preprocessor\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'vehicle_trim_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/workspace/classified_embeddings_v2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'vehicle_trim_level'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dataset_preprocessor = DatasetPreprocessor(numerical_columns_names, categorical_columns_names)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdataset_preprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mDatasetPreprocessor.fit\u001b[39m\u001b[34m(self, dataframe)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitting dataset preprocessor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.numerical_columns = NumericalColumns.from_names(dataframe, columns=\u001b[38;5;28mself\u001b[39m.numerical_columns_names)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.categorical_columns = \u001b[43mCategoricalColumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategorical_columns_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mCategoricalColumns.from_names\u001b[39m\u001b[34m(cls, dataframe, columns)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_names\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dataframe: pd.DataFrame, columns: List[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mCategoricalColumns\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m         columns={column: CategoricalColumn.from_series(\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns}\n\u001b[32m     70\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/workspace/classified_embeddings_v2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/workspace/classified_embeddings_v2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'vehicle_trim_level'"
     ]
    }
   ],
   "source": [
    "dataset_preprocessor = DatasetPreprocessor(numerical_columns_names, categorical_columns_names)\n",
    "dataset_preprocessor.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = NumericalColumns.from_names(dataframe, columns=numerical_columns_names)\n",
    "categorical_columns = CategoricalColumns.from_names(dataframe, columns=categorical_columns_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m HIDDEN_LAYERS_DIM = [\u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m32\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Encoding part\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m numerical_inputs_layer = Input(shape=(\u001b[38;5;28mlen\u001b[39m(\u001b[43mnumerical_columns\u001b[49m.columns),), name=\u001b[33m\"\u001b[39m\u001b[33mnumerical_inputs_features\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m numerical_inputs_layer = BatchNormalization()(numerical_inputs_layer)\n\u001b[32m     10\u001b[39m inputs[\u001b[33m\"\u001b[39m\u001b[33mnumerical_inputs_features\u001b[39m\u001b[33m\"\u001b[39m] = numerical_inputs_layer\n",
      "\u001b[31mNameError\u001b[39m: name 'numerical_columns' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = {}\n",
    "embeddings = []\n",
    "\n",
    "FINAL_BOTTLENECK_DIM = 32\n",
    "HIDDEN_LAYERS_DIM = [128, 64, 32]\n",
    "\n",
    "# Encoding part\n",
    "numerical_inputs_layer = Input(shape=(len(numerical_columns.columns),), name=\"numerical_inputs_features\")\n",
    "numerical_inputs_layer = BatchNormalization()(numerical_inputs_layer)\n",
    "inputs[\"numerical_inputs_features\"] = numerical_inputs_layer\n",
    "embeddings.append(numerical_inputs_layer)\n",
    "\n",
    "for feature_name, feature in categorical_columns.columns.items():\n",
    "    categorical_input_layer = Input(shape=(1,), name=feature_name)\n",
    "    inputs[feature_name] = categorical_input_layer\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=len(feature.vocabulary) + 1,\n",
    "        output_dim=feature.embedding_dim,\n",
    "        name=f\"{feature_name}_embedding\"\n",
    "    )(categorical_input_layer)\n",
    "\n",
    "    embedding_layer = Flatten(name=f\"{feature_name}_embedding_flatten\")(embedding_layer)\n",
    "    embeddings.append(embedding_layer)\n",
    "\n",
    "all_features_layer = Concatenate()(embeddings)\n",
    "\n",
    "for index, hidden_layer_dim in enumerate(HIDDEN_LAYERS_DIM):\n",
    "    all_features_layer = Dense(units=hidden_layer_dim, activation=\"relu\", name=f\"hidden_layer_{index}\")(all_features_layer)\n",
    "\n",
    " \n",
    "bottleneck_layer = Dense(units=FINAL_BOTTLENECK_DIM, activation=\"tanh\", name=\"bottleneck_layer\")(all_features_layer)\n",
    "\n",
    "# Decoding part\n",
    "\n",
    "first_decoding_layer = Dense(units=64, activation=\"relu\", name=\"first_decoding_layer\")(bottleneck_layer)\n",
    "\n",
    "for index, hidden_layer_dim in enumerate(reversed(HIDDEN_LAYERS_DIM)):\n",
    "    first_decoding_layer = Dense(units=hidden_layer_dim, activation=\"relu\", name=f\"decoding_layer_{index}\")(first_decoding_layer)\n",
    "    first_decoding_layer = Dropout(0.2)(first_decoding_layer)\n",
    "\n",
    "outputs = {}\n",
    "\n",
    "numerical_outputs = Dense(units=len(numerical_columns.columns), name=\"numerical_outputs\")(first_decoding_layer)\n",
    "outputs[\"numerical_outputs\"] = numerical_outputs\n",
    "\n",
    "for feature_name, feature in categorical_columns.columns.items():\n",
    "    categorical_output_layer = Dense(units=len(feature.vocabulary) + 1, name=f\"{feature_name}_outputs\", activation=\"softmax\")(first_decoding_layer)\n",
    "    outputs[f\"{feature_name}_outputs\"] = categorical_output_layer\n",
    "\n",
    "autoencoder = Model(inputs=inputs, outputs=outputs)\n",
    "encoder = Model(inputs=inputs, outputs=bottleneck_layer)\n",
    "\n",
    "\n",
    "losses = {}\n",
    "loss_weights = {}\n",
    "\n",
    "losses[\"numerical_outputs\"] = \"mse\"\n",
    "loss_weights[\"numerical_outputs\"] = 1.0\n",
    "\n",
    "for feature_name in categorical_columns.columns:\n",
    "    losses[f\"{feature_name}_outputs\"] = \"sparse_categorical_crossentropy\"\n",
    "    loss_weights[f\"{feature_name}_outputs\"] = 1.0\n",
    "\n",
    "autoencoder.compile(optimizer=\"adam\", loss=losses, loss_weights=loss_weights)\n",
    "autoencoder.save(\"autoencoder.keras\")\n",
    "model = load_model(\"autoencoder.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['vehicle_trim_level'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x = \u001b[43mdataset_preprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m y = dataset_preprocessor.preprocess_target(dataframe)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mDatasetPreprocessor.preprocess\u001b[39m\u001b[34m(self, dataframe)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe: pd.DataFrame) -> Dict[\u001b[38;5;28mstr\u001b[39m, pd.Series]:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     transformed_data = \u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumerical_columns_names\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategorical_columns_names\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numerical_columns.columns:\n\u001b[32m     32\u001b[39m         transformed_data[column] = \u001b[38;5;28mself\u001b[39m.numerical_columns.columns[column].transform(transformed_data[column])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/workspace/classified_embeddings_v2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/workspace/classified_embeddings_v2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/workspace/classified_embeddings_v2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['vehicle_trim_level'] not in index\""
     ]
    }
   ],
   "source": [
    "x = dataset_preprocessor.preprocess(dataframe)\n",
    "y = dataset_preprocessor.preprocess_target(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mautoencoder\u001b[49m.fit(x=x, y=y, epochs=\u001b[32m10\u001b[39m, batch_size=\u001b[32m100\u001b[39m, validation_split=\u001b[32m0.2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(x=x, y=y, epochs=10, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.9397067e-01,  9.6709681e-01,  7.9613245e-01, ...,\n",
       "        -1.7263100e-01,  6.7366630e-01, -1.4135163e-02],\n",
       "       [ 9.3868989e-01,  9.8516482e-01,  9.1622311e-01, ...,\n",
       "        -2.9181448e-01,  8.3701688e-01, -1.4009577e-01],\n",
       "       [ 8.8373524e-01,  9.5905167e-01,  7.8330857e-01, ...,\n",
       "        -1.5059380e-01,  6.3525021e-01,  1.7593242e-04],\n",
       "       ...,\n",
       "       [ 9.3370491e-01,  9.7682655e-01,  8.9947945e-01, ...,\n",
       "        -2.0524356e-01,  8.1432486e-01, -1.5843923e-01],\n",
       "       [ 7.3241997e-01,  8.7240243e-01,  6.4386541e-01, ...,\n",
       "        -1.5077646e-02,  5.8866918e-01,  1.6981578e-02],\n",
       "       [ 9.7714382e-01,  9.9614549e-01,  9.7372222e-01, ...,\n",
       "        -1.9887161e-01,  9.2300165e-01, -3.5595196e-01]], dtype=float32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.09129161, -0.3003128 ,  0.14279275, -0.0788637 , -0.20313236,\n",
       "         0.16591106,  0.23145911, -0.4382897 , -0.11219118, -0.11739425,\n",
       "        -0.17356685, -0.3678083 , -0.12830657,  0.19102648, -0.2664427 ,\n",
       "        -0.06730663,  0.41341332,  0.37312272, -0.09601668, -0.01168428,\n",
       "         0.1544762 , -0.08641091, -0.00380521,  0.13412751, -0.07791837,\n",
       "         0.5365318 ,  0.531186  , -0.08955383,  0.05663909,  0.40029883,\n",
       "        -0.31981412,  0.22963767]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.predict(dataset_preprocessor.preprocess(prediction_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
